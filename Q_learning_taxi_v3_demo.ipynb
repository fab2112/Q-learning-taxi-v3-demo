{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q-learning_taxi-v3_demo.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMbvmJwDSGpVmvYpUOin96x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fab2112/Q-learning-taxi-v3-demo/blob/main/Q_learning_taxi_v3_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDhfteWZO9W8"
      },
      "source": [
        "# Demonstração do algorítmo Q-learning no ambiente *taxi-v3* *openai* *gym*\n",
        "https://gym.openai.com/envs/Taxi-v3/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N41oOI092hf6"
      },
      "source": [
        "# Importa bibliotecas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-EbaV9FxwJv"
      },
      "source": [
        "from time import sleep, time\n",
        "import numpy as np\n",
        "import gym.spaces\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import seaborn as sns"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ItaI_H52O6U"
      },
      "source": [
        "# Define parâmetros, instancia ambiente e inicializa Q_table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZTItgzfyCBY"
      },
      "source": [
        "# Instancia ambiente Taxi-v3 no gym\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "\n",
        "# Inicializa tabela Q\n",
        "Q_table = np.random.random([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "# Parâmetros Q-learning\n",
        "gamma = 0.2\n",
        "alpha = 0.4\n",
        "epsilon = 0.2\n",
        "epsilon_decay = 0.99\n",
        "episodes = 20000\n",
        "\n",
        "total_epochs = 0\n",
        "total_rewards = []"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzzem97c3krD"
      },
      "source": [
        "# Fase de treino"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhhwb3JSyGyX"
      },
      "source": [
        "for episode in range(episodes):\n",
        "\n",
        "    time_before = float(time() * 1000)\n",
        "\n",
        "    reward = 0\n",
        "    epochs = 0\n",
        "    rewards_by_episode = 0\n",
        "    state = env.reset()\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "\n",
        "        # Seleciona uma ação aleatória - Exploration\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        # Seleciona uma ação com maior valor Q - Exploitation\n",
        "        else:\n",
        "            action = np.argmax(Q_table[state])\n",
        "\n",
        "        # Move para próximo estado e atualiza recompensa\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Atualiza valor Q(s, a) - Equação de Bellman\n",
        "        Q_table[state, action] = \\\n",
        "            Q_table[state, action] + alpha * (reward + gamma * (np.max(Q_table[next_state])) - Q_table[state, action])\n",
        "\n",
        "        # Atualiza state para o estado atual\n",
        "        state = next_state\n",
        "        # Computa rewards por episodio\n",
        "        rewards_by_episode += reward\n",
        "        # Computa epocas por episodio\n",
        "        epochs += 1\n",
        "\n",
        "        # Render treino\n",
        "        #env.render()\n",
        "        #sleep(0.06)\n",
        "\n",
        "    # Monitoramento da latência do treino por espiósdio\n",
        "    time_after = float(time() * 1000)\n",
        "    latency = \"%.2f\" % (time_after - time_before)\n",
        "    #print(\"Episode \" + str(episode) + \" Latencia: \" + str(latency) + \" ms\")\n",
        "\n",
        "    # Decaimento de epsilon - Exploitation\n",
        "    epsilon = epsilon * epsilon_decay\n",
        "\n",
        "    total_epochs += epochs\n",
        "    total_rewards.append(rewards_by_episode)\n",
        "    #clear_output(wait=True)\n",
        "    #print(\"Rewards by episodes: {}\".format(rewards_by_episode))\n",
        "\n",
        "\n",
        "print(\"\\nSuccess rate epochs by episodes: {}\".format(total_epochs / episodes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWVhSdyv3yvI"
      },
      "source": [
        "# Plota rewards x  episodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7Afl3_gyNLq"
      },
      "source": [
        "# Plot total_rewards\n",
        "sns.set_theme()\n",
        "plt.clf()\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.plot(total_rewards, marker='.', linestyle='none')\n",
        "plt.title('Q-learning Agente')\n",
        "plt.ylabel('Rewards')\n",
        "plt.xlabel('Episodes')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SArwG1l23_Rq"
      },
      "source": [
        "# Fase de teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w88h_9jpyUJa"
      },
      "source": [
        "# Após treino\n",
        "# Cria novo ambiente e vai para um estado aleatório\n",
        "state = env.reset()\n",
        "# Seleciona a ação indexada máximo valor do vetor da tabela Q_table\n",
        "action = np.argmax(Q_table[state])\n",
        "for i in range(100):\n",
        "    sleep(0.2)\n",
        "    # Define o novo estado em função da ultima ação escolhida\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    # Seleciona a ação indexada máximo valor do vetor Q_table\n",
        "    action = np.argmax(Q_table[next_state])\n",
        "    clear_output(wait=True)\n",
        "    env.render()\n",
        "    if reward >= 20:\n",
        "        print(\"\\n\\nNOVO AMBIENTE\\n\\n\")\n",
        "        sleep(0.5)\n",
        "        reward = 0\n",
        "        # Cria novo ambiente e vai para um estado aleatório\n",
        "        state = env.reset()\n",
        "        # Seleciona a ação indexada máximo valor do vetor da tabela Q_table\n",
        "        action = np.argmax(Q_table[state])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}